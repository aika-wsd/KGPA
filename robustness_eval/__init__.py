# 用于评估机器学习模型对于基于提示的对抗性攻击的鲁棒性，特别是针对大型语言模型（LLM）

"""
主要用于评估机器学习模型对于基于提示的对抗性攻击的鲁棒性，特别是针对大型语言模型（LLM）。
以下是脚本的主要组成部分和功能：

导入和依赖：
脚本导入了必要的库，如os、pickle、argparse、tqdm、numpy和concurrent.futures。
还从外部文件导入了Predict、load_dataset和PromptAttack模块，用于预测和生成攻击的自定义模块。

命令行参数设置：
使用argparse.ArgumentParser来处理命令行参数。这些参数配置攻击和评估过程的各个方面，
如API密钥、数据集选择、干扰阈值（tau值）、批处理大小以及日志保存路径。

大型语言模型（LLM）配置：
脚本中包含了注释掉的部分，用于设置LLM的基础URL和版本，显示了不同API端点和模型版本之间的可切换配置。

评估指标：
脚本定义了词汇修改率（tau_1）和BERT得分（tau_2）的阈值，用来控制攻击的严重性和隐蔽性。

主要评估循环：
处理数据集（如SST-2, QQP, MNLI）时，会遍历数据批次。对每个批次进行自然（未干扰）和对抗（干扰）预测。
使用PromptAttack类的批处理攻击函数生成对抗性样本，该函数接受干扰类型和阈值等参数。

计算准确率和攻击成功率：
脚本计算自然准确率和鲁棒准确率以及攻击成功率（ASR），以评估模型在对抗条件下的表现。
每个任务描述的结果会被打印并在最后进行总结。

该脚本用于研究或测试环境中，以测量不同配置的语言模型在面对故意制造的、旨在混淆或诱导模型做出错误预测的输入时的处理能力。
"""
